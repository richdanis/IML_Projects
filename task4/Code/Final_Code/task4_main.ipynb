{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task4_main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ADu3Y6Qmy8A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time\n",
        "import random\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix seeds\n",
        "torch.manual_seed(13)\n",
        "random.seed(13)\n",
        "np.random.seed(13)"
      ],
      "metadata": {
        "id": "cptL_faWp6d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "oigU1zHLm0u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive to access data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r0LpYh8Wp9rf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170c976a-4c8f-4384-9f0c-15a37cae657b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move data to working directory and import our library\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('drive/MyDrive/Data.zip','r') as zipObj:\n",
        "  zipObj.extractall('.')\n",
        "\n",
        "!cp drive/MyDrive/task4_lib.py .\n",
        "\n",
        "from task4_lib import *"
      ],
      "metadata": {
        "id": "p9bb5RlLqAwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model definition for pretrain: autoencoder and regressor in one model\n",
        "# optimize for both reconstruction loss and regression loss\n",
        "# at the same time to get good compressed feature representation\n",
        "class MolecularNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(1000, 900),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(900, 500),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(500, 20),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(20, 500),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(500, 900),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(900, 1000),\n",
        "        )\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(20, 1)\n",
        "        )\n",
        "\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        self.reconstruction_loss = None\n",
        "        self.regression_loss = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        # calculate loss for autoencoder\n",
        "\n",
        "        f = self.encoder(x)\n",
        "        d = self.decoder(f)\n",
        "\n",
        "        self.reconstruction_loss = 1.5 * self.loss_fn(x,d)\n",
        "\n",
        "        # calculate loss for regressor\n",
        "        \n",
        "        r = self.regressor(f)\n",
        "\n",
        "        self.regression_loss = self.loss_fn(r,y)\n",
        "\n",
        "        # return sum of both losses\n",
        "        \n",
        "        return self.reconstruction_loss + self.regression_loss"
      ],
      "metadata": {
        "id": "qtjAa0XVp7me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get dataloaders for pretrain dataset\n",
        "# function definition in task4_lib.py\n",
        "# pretrain_loader and preval_loader split the dataset 80/20\n",
        "# prefull_loader goes over the whole pretrain dataset\n",
        "pretrain_loader, preval_loader, prefull_loader = get_loaders(dataset=\"pretrain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PjzsZBCNq4H",
        "outputId": "a915ce48-431f-44a4-a268-598e36e34ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model declaration and optimizer\n",
        "moc = MolecularNet().to(device)\n",
        "optim = torch.optim.Adam(moc.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "loPvjZ4OGr3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train loop for the pretrain dataset, definition in task4_lib.py\n",
        "# keep track of reconstruction and regression loss\n",
        "# when filling in prefull_loader in for both train_loader and val_loader, then\n",
        "# training is going over the whole dataset (in this case the validation scores\n",
        "# are meaningless)\n",
        "pre_train_loop(moc, pretrain_loader, preval_loader, optim, device, show=1, save=6, epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "SYovY6pQHaTr",
        "outputId": "0dcfcccc-e2ca-4853-9b97-962a803bccd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Train reconstruction loss: 0.0463 |  Train regression loss: 0.3160 |  Validation reconstruction loss: 0.0371 |  Validation regression loss: 0.1127 |  Duration 4.18 sec\n",
            "Epoch 1 | Train reconstruction loss: 0.0314 |  Train regression loss: 0.0947 |  Validation reconstruction loss: 0.0269 |  Validation regression loss: 0.0959 |  Duration 5.28 sec\n",
            "Epoch 2 | Train reconstruction loss: 0.0239 |  Train regression loss: 0.0849 |  Validation reconstruction loss: 0.0219 |  Validation regression loss: 0.0974 |  Duration 4.93 sec\n",
            "Epoch 3 | Train reconstruction loss: 0.0203 |  Train regression loss: 0.0813 |  Validation reconstruction loss: 0.0193 |  Validation regression loss: 0.0813 |  Duration 4.03 sec\n",
            "Epoch 4 | Train reconstruction loss: 0.0183 |  Train regression loss: 0.0758 |  Validation reconstruction loss: 0.0178 |  Validation regression loss: 0.0866 |  Duration 4.02 sec\n",
            "Epoch 5 | Train reconstruction loss: 0.0169 |  Train regression loss: 0.0697 |  Validation reconstruction loss: 0.0169 |  Validation regression loss: 0.0735 |  Duration 4.01 sec\n",
            "Epoch 6 | Train reconstruction loss: 0.0159 |  Train regression loss: 0.0689 |  Validation reconstruction loss: 0.0158 |  Validation regression loss: 0.0799 |  Duration 3.98 sec\n",
            "Saved model checkpoint to model_epoch_6.pt\n",
            "Epoch 7 | Train reconstruction loss: 0.0150 |  Train regression loss: 0.0639 |  Validation reconstruction loss: 0.0149 |  Validation regression loss: 0.0681 |  Duration 4.01 sec\n",
            "Epoch 8 | Train reconstruction loss: 0.0142 |  Train regression loss: 0.0614 |  Validation reconstruction loss: 0.0144 |  Validation regression loss: 0.0750 |  Duration 4.01 sec\n",
            "Epoch 9 | Train reconstruction loss: 0.0136 |  Train regression loss: 0.0584 |  Validation reconstruction loss: 0.0137 |  Validation regression loss: 0.0680 |  Duration 4.01 sec\n",
            "Epoch 10 | Train reconstruction loss: 0.0130 |  Train regression loss: 0.0549 |  Validation reconstruction loss: 0.0133 |  Validation regression loss: 0.0647 |  Duration 3.94 sec\n",
            "Epoch 11 | Train reconstruction loss: 0.0125 |  Train regression loss: 0.0541 |  Validation reconstruction loss: 0.0129 |  Validation regression loss: 0.0616 |  Duration 3.92 sec\n",
            "Epoch 12 | Train reconstruction loss: 0.0120 |  Train regression loss: 0.0499 |  Validation reconstruction loss: 0.0128 |  Validation regression loss: 0.0697 |  Duration 3.95 sec\n",
            "Saved model checkpoint to model_epoch_12.pt\n",
            "Epoch 13 | Train reconstruction loss: 0.0116 |  Train regression loss: 0.0502 |  Validation reconstruction loss: 0.0122 |  Validation regression loss: 0.0606 |  Duration 3.97 sec\n",
            "Epoch 14 | Train reconstruction loss: 0.0111 |  Train regression loss: 0.0447 |  Validation reconstruction loss: 0.0118 |  Validation regression loss: 0.0862 |  Duration 3.90 sec\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e9ebb15bb189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# training is going over the whole dataset (in this case the validation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# are meaningless)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpre_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/task4_lib.py\u001b[0m in \u001b[0;36mpre_train_loop\u001b[0;34m(model, train_loader, val_loader, optim, device, show, save, epochs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# zero grads and put model into train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# forward pass and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1733\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1733\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training mode is expected to be boolean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdicts_or_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts_or_sets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model definition for the regression of HOMO-LUMO gap\n",
        "class GapRegressor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = None\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(20, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.encoder(x)\n",
        "        x = self.regressor(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "x9H9wl_yyM_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load encoder from pretrained model and freeze the weights\n",
        "combined = torch.load(\"model_epoch_12.pt\")\n",
        "encoder = combined.encoder\n",
        "for param in encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# gap regressor declaration, optimizer and loss function\n",
        "reg = GapRegressor()\n",
        "reg.encoder = encoder\n",
        "reg = reg.to(device)\n",
        "\n",
        "optim = torch.optim.Adam(reg.parameters(), lr=1e-3)\n",
        "\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "usGqGiRgIs0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get dataloaders for train dataset\n",
        "# function definition in task4_lib.py\n",
        "# train_loader and val_loader split the dataset 80/20\n",
        "# full_loader goes over the whole pretrain dataset\n",
        "train_loader, val_loader, full_loader = get_loaders(dataset=\"train\",batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHfRPjTA0AKo",
        "outputId": "3575b106-b725-4029-ebbb-94be92fe83ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# second train loop for train dataset, definition in task4_lib.py\n",
        "# keep track of regression loss\n",
        "# when filling in prefull_loader in for both train_loader and val_loader, then\n",
        "# training is going over the whole dataset (in this case the validation scores\n",
        "# are meaningless)\n",
        "train_loop(reg, full_loader, full_loader, loss_fn, optim, device, show=100, save=5000, epochs=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ix7-6EHsJcEs",
        "outputId": "748061a7-c6ea-485f-ea00-1e38d73bf0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Train loss: 1.1409 |  Validation loss: 1.0306 |  Duration 0.03 sec\n",
            "Epoch 100 | Train loss: 0.4547 |  Validation loss: 0.4541 |  Duration 0.03 sec\n",
            "Epoch 200 | Train loss: 0.4049 |  Validation loss: 0.4036 |  Duration 0.02 sec\n",
            "Epoch 300 | Train loss: 0.3677 |  Validation loss: 0.3674 |  Duration 0.02 sec\n",
            "Epoch 400 | Train loss: 0.3399 |  Validation loss: 0.3379 |  Duration 0.02 sec\n",
            "Epoch 500 | Train loss: 0.3148 |  Validation loss: 0.3137 |  Duration 0.02 sec\n",
            "Epoch 600 | Train loss: 0.2927 |  Validation loss: 0.2924 |  Duration 0.02 sec\n",
            "Epoch 700 | Train loss: 0.2752 |  Validation loss: 0.2738 |  Duration 0.02 sec\n",
            "Epoch 800 | Train loss: 0.2581 |  Validation loss: 0.2572 |  Duration 0.03 sec\n",
            "Epoch 900 | Train loss: 0.2422 |  Validation loss: 0.2414 |  Duration 0.03 sec\n",
            "Epoch 1000 | Train loss: 0.2284 |  Validation loss: 0.2264 |  Duration 0.02 sec\n",
            "Epoch 1100 | Train loss: 0.2138 |  Validation loss: 0.2131 |  Duration 0.02 sec\n",
            "Epoch 1200 | Train loss: 0.2031 |  Validation loss: 0.2011 |  Duration 0.02 sec\n",
            "Epoch 1300 | Train loss: 0.1907 |  Validation loss: 0.1901 |  Duration 0.03 sec\n",
            "Epoch 1400 | Train loss: 0.1813 |  Validation loss: 0.1804 |  Duration 0.02 sec\n",
            "Epoch 1500 | Train loss: 0.1735 |  Validation loss: 0.1720 |  Duration 0.03 sec\n",
            "Epoch 1600 | Train loss: 0.1651 |  Validation loss: 0.1647 |  Duration 0.02 sec\n",
            "Epoch 1700 | Train loss: 0.1597 |  Validation loss: 0.1589 |  Duration 0.02 sec\n",
            "Epoch 1800 | Train loss: 0.1540 |  Validation loss: 0.1537 |  Duration 0.03 sec\n",
            "Epoch 1900 | Train loss: 0.1519 |  Validation loss: 0.1511 |  Duration 0.02 sec\n",
            "Epoch 2000 | Train loss: 0.1475 |  Validation loss: 0.1466 |  Duration 0.02 sec\n",
            "Epoch 2100 | Train loss: 0.1443 |  Validation loss: 0.1437 |  Duration 0.02 sec\n",
            "Epoch 2200 | Train loss: 0.1442 |  Validation loss: 0.1417 |  Duration 0.02 sec\n",
            "Epoch 2300 | Train loss: 0.1438 |  Validation loss: 0.1418 |  Duration 0.03 sec\n",
            "Epoch 2400 | Train loss: 0.1403 |  Validation loss: 0.1401 |  Duration 0.02 sec\n",
            "Epoch 2500 | Train loss: 0.1391 |  Validation loss: 0.1391 |  Duration 0.02 sec\n",
            "Epoch 2600 | Train loss: 0.1392 |  Validation loss: 0.1379 |  Duration 0.03 sec\n",
            "Epoch 2700 | Train loss: 0.1380 |  Validation loss: 0.1380 |  Duration 0.02 sec\n",
            "Epoch 2800 | Train loss: 0.1373 |  Validation loss: 0.1373 |  Duration 0.02 sec\n",
            "Epoch 2900 | Train loss: 0.1371 |  Validation loss: 0.1372 |  Duration 0.03 sec\n",
            "Epoch 3000 | Train loss: 0.1373 |  Validation loss: 0.1365 |  Duration 0.02 sec\n",
            "Epoch 3100 | Train loss: 0.1376 |  Validation loss: 0.1365 |  Duration 0.02 sec\n",
            "Epoch 3200 | Train loss: 0.1386 |  Validation loss: 0.1363 |  Duration 0.03 sec\n",
            "Epoch 3300 | Train loss: 0.1382 |  Validation loss: 0.1362 |  Duration 0.02 sec\n",
            "Epoch 3400 | Train loss: 0.1366 |  Validation loss: 0.1367 |  Duration 0.02 sec\n",
            "Epoch 3500 | Train loss: 0.1376 |  Validation loss: 0.1365 |  Duration 0.02 sec\n",
            "Epoch 3600 | Train loss: 0.1391 |  Validation loss: 0.1368 |  Duration 0.03 sec\n",
            "Epoch 3700 | Train loss: 0.1380 |  Validation loss: 0.1369 |  Duration 0.02 sec\n",
            "Epoch 3800 | Train loss: 0.1372 |  Validation loss: 0.1361 |  Duration 0.02 sec\n",
            "Epoch 3900 | Train loss: 0.1398 |  Validation loss: 0.1360 |  Duration 0.02 sec\n",
            "Epoch 4000 | Train loss: 0.1374 |  Validation loss: 0.1360 |  Duration 0.02 sec\n",
            "Epoch 4100 | Train loss: 0.1393 |  Validation loss: 0.1365 |  Duration 0.02 sec\n",
            "Epoch 4200 | Train loss: 0.1366 |  Validation loss: 0.1361 |  Duration 0.02 sec\n",
            "Epoch 4300 | Train loss: 0.1367 |  Validation loss: 0.1365 |  Duration 0.03 sec\n",
            "Epoch 4400 | Train loss: 0.1383 |  Validation loss: 0.1373 |  Duration 0.03 sec\n",
            "Epoch 4500 | Train loss: 0.1359 |  Validation loss: 0.1371 |  Duration 0.04 sec\n",
            "Epoch 4600 | Train loss: 0.1376 |  Validation loss: 0.1360 |  Duration 0.02 sec\n",
            "Epoch 4700 | Train loss: 0.1369 |  Validation loss: 0.1362 |  Duration 0.03 sec\n",
            "Epoch 4800 | Train loss: 0.1369 |  Validation loss: 0.1360 |  Duration 0.03 sec\n",
            "Epoch 4900 | Train loss: 0.1373 |  Validation loss: 0.1370 |  Duration 0.02 sec\n",
            "Epoch 5000 | Train loss: 0.1362 |  Validation loss: 0.1368 |  Duration 0.03 sec\n",
            "Saved model checkpoint to model_epoch_5000.pt\n",
            "Epoch 5100 | Train loss: 0.1380 |  Validation loss: 0.1361 |  Duration 0.02 sec\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6e8cfb7d07b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# training is going over the whole dataset (in this case the validation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# are meaningless)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/task4_lib.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_loader, val_loader, loss_fn, optim, device, show, save, epochs)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;31m# backward pass and gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# keep track of train stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                    maximize=group['maximize'])\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store model\n",
        "!cp model_epoch_5000.pt drive/MyDrive/molecular_epoch_5000.pt"
      ],
      "metadata": {
        "id": "7d40dJ4YJlOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r6FUy6FtRnS7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}